# ğŸ§ Environmental Audio Analysis, Separation, and Enhancement Pipeline

## Overview
This project implements a **deep learning-based end-to-end system** for robust analysis, separation, and enhancement of environmental audio.  
It supports both **single-channel (mono)** and **multi-channel (microphone array)** inputs, dynamically adapting its processing pipeline based on input configuration.

The system outputs:
- A **list of classified audio events** with timestamps (and spatial coordinates if multi-channel).
- A set of **individually separated and enhanced audio files (WAV)** corresponding to each detected source.

---

## ğŸ”‘ Key Features
- **Adaptive Input Handling**: Automatically detects channel count and configures pipeline accordingly.
- **Deep Denoising**: Noise reduction via DAE or WaveNet-based models.
- **Source Localization (Multi-channel)**: SELD network estimates DOA (azimuth, elevation).
- **Event Classification & Detection**: CRNN or Transformer-based architectures for strong event detection.
- **Adaptive Source Separation**:
  - Multi-channel: Spatially-informed beamforming + mask networks.
  - Mono: Blind source separation (Conv-TasNet / SepFormer).
- **Class-Specific Enhancement**: Modular enhancement tailored to event type (speech, gunshot, siren, etc.).

---

## ğŸ“¦ Pipeline Architecture

### 1. âš™ï¸ Adaptive Input & Pre-processing
- Detects channel count (C).
- Denoises audio using DAE/WaveNet.
- Normalizes and resamples input.

### 2. ğŸ“ Audio Source Localization (C > 1 only)
- SELD network trained on microphone array data.
- Outputs event timestamps, labels, and DOA (Ï•, Î¸).

### 3. ğŸ·ï¸ Event Classification & Detection
- CRNN or Transformer-based classifier.
- Outputs `(Class Label, Start Time, End Time)`.

### 4. ğŸ”— Adaptive Source Separation
- **Multi-channel**: Beamforming + DOA-informed mask networks.
- **Mono**: Blind source separation (Conv-TasNet / SepFormer).
- Outputs N separated waveform files.

### 5. âœ¨ Class-Specific Enhancement
- Speech â†’ SEGAN/DNS-style enhancement.
- Gunshot/Explosion â†’ Transient restoration.
- Siren/Alarm â†’ Harmonic enhancement.
- Car engine â†’ Low-frequency stabilization.

---

## ğŸ—‚ï¸ Output
- **Event List**: JSON or CSV containing detected events.
- **Separated Audio**: WAV files named by event type and timestamp.

Example:
- events.json â”œâ”€â”€ [ { "class": "Speech", "start": 1.2, "end": 4.5 } ] outputs/ â”œâ”€â”€ 0001_speech.wav â”œâ”€â”€ 0002_siren.wav


---

## âš™ï¸ Installation

### Requirements
- Python 3.9+
- PyTorch / TensorFlow (depending on chosen models)
- Librosa, NumPy, SciPy
- Soundfile, PyTorch Lightning (recommended)

### Setup
```bash
git clone https://github.com/your-repo/audio-pipeline.git
cd audio-pipeline
pip install -r requirements.txt


### ğŸš€ Usage
Run pipeline on an audio file
python run_pipeline.py --input input.wav --output_dir outputs/


Options
- --array_geometry: JSON file with microphone positions (required for multi-channel).
- --sample_rate: Target sample rate (default: 24000).
- --model_config: Path to YAML config for model selection.

ğŸ“Š Evaluation Metrics
- Denoising: SI-SDR, SI-SNR.
- Localization: DOA error (degrees).
- Classification: Event-based F1, segment-based F1.
- Separation: SI-SDR, SDR, SIR, SAR.
- Enhancement: PESQ, STOI, transient sharpness indices.

ğŸ› ï¸ Project Structure
audio-pipeline/
â”œâ”€â”€ configs/              # YAML configs for models
â”œâ”€â”€ checkpoints/          # Pretrained weights
â”œâ”€â”€ data/                 # Sample datasets
â”œâ”€â”€ modules/              # Core pipeline modules
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ localization.py
â”‚   â”œâ”€â”€ classification.py
â”‚   â”œâ”€â”€ separation.py
â”‚   â””â”€â”€ enhancement.py
â”œâ”€â”€ run_pipeline.py       # Main entry point
â””â”€â”€ README.md



ğŸ“š Training Data
- Multi-channel: DCASE SELD datasets.
- Mono: ESC-50, UrbanSound8K, LibriSpeech, DNS Challenge.
- Synthetic mixtures: Augmented with impulse responses and noise.

ğŸ¤ Contributing
Contributions are welcome!
- Fork the repo
- Create a feature branch
- Submit a pull request

ğŸ“œ License
MIT License. See LICENSE file for details.

ğŸŒŸ Future Work
- Real-time inference optimization.
- Expanded taxonomy of environmental sounds.
- Unified enhancement model for cross-class generalization.

---

Would you like me to also draft a **minimal repo skeleton with `run_pipeline.py` and configs** so you can drop this README in and have a working starter structure? That way, youâ€™d have both documentation and code scaffolding ready to go.

î·™î·š



## to run

# 1st

# (optional) activate your venv if you use it
# .\g\venv\Scripts\Activate.ps1

# install CPU wheel
# python -m pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu

## Then rerun:
# (python cp\cp1.py --input path\to\file.wav)